{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Hyper Parameteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.99\n",
    "lr=1e-3\n",
    "min_episodes=20\n",
    "eps=1\n",
    "eps_decay=0.995\n",
    "eps_min=0.01\n",
    "update_step=10\n",
    "batch_size=64\n",
    "update_repeats=50\n",
    "num_episodes=3000\n",
    "seed=42\n",
    "max_memory_size=50000\n",
    "lr_gamma=0.9\n",
    "lr_step=100\n",
    "measure_step=100\n",
    "measure_repeats=100\n",
    "hidden_dim=64\n",
    "horizon=np.inf\n",
    "render=True\n",
    "render_step=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env =gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inderstanding the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action_space:  Discrete(2) \n",
      "\n",
      "Number of actions:  2 \n",
      "\n",
      "Action_space sample: 1 \n",
      "\n",
      "Observation_space:  Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32) \n",
      "\n",
      "State shape:  (4,) \n",
      "\n",
      "Observation_space sample: [-3.4024484e+00 -2.5514683e+38 -2.3285833e-01  1.3241576e+38] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Action_space: ',env.action_space,\"\\n\")\n",
    "print('Number of actions: ', env.action_space.n,\"\\n\")\n",
    "print('Action_space sample:',env.action_space.sample(),\"\\n\")\n",
    "print('Observation_space: ',env.observation_space,\"\\n\")\n",
    "print('State shape: ', env.observation_space.shape,\"\\n\")\n",
    "print('Observation_space sample:',env.observation_space.sample(),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Untrained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:1 score:12.0\n",
      "episode:2 score:15.0\n",
      "episode:3 score:19.0\n",
      "episode:4 score:44.0\n",
      "episode:5 score:17.0\n"
     ]
    }
   ],
   "source": [
    "episodes=5\n",
    "for episode in range(1,episodes+1):\n",
    "    state=env.reset()\n",
    "    done=False\n",
    "    score=0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action=env.action_space.sample()\n",
    "        n_state,reward,done,info=env.step(action)\n",
    "        score+=reward\n",
    "    print(\"episode:{} score:{}\".format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an agent using Double DQN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, action_dim, state_dim, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        x1 = F.leaky_relu(self.fc_1(inp))\n",
    "        x1 = F.leaky_relu(self.fc_2(x1))\n",
    "        x1 = self.fc_3(x1)\n",
    "\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, len):\n",
    "        self.rewards = collections.deque(maxlen=len)\n",
    "        self.state = collections.deque(maxlen=len)\n",
    "        self.action = collections.deque(maxlen=len)\n",
    "        self.is_done = collections.deque(maxlen=len)\n",
    "\n",
    "    def update(self, state, action, reward, done):\n",
    "        # if the episode is finished we do not save to new state. Otherwise we have more states per episode than rewards\n",
    "        # and actions whcih leads to a mismatch when we sample from memory.\n",
    "        if not done:\n",
    "            self.state.append(state)\n",
    "        self.action.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.is_done.append(done)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        sample \"batch_size\" many (state, action, reward, next state, is_done) datapoints.\n",
    "        \"\"\"\n",
    "        n = len(self.is_done)\n",
    "        idx = random.sample(range(0, n-1), batch_size)\n",
    "\n",
    "        return torch.Tensor(self.state)[idx].to(device), torch.LongTensor(self.action)[idx].to(device), \\\n",
    "               torch.Tensor(self.state)[1+np.array(idx)].to(device), torch.Tensor(self.rewards)[idx].to(device), \\\n",
    "               torch.Tensor(self.is_done)[idx].to(device)\n",
    "\n",
    "    def reset(self):\n",
    "        self.rewards.clear()\n",
    "        self.state.clear()\n",
    "        self.action.clear()\n",
    "        self.is_done.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(model, env, state, eps):\n",
    "    state = torch.Tensor(state).to(device)\n",
    "    with torch.no_grad():\n",
    "        values = model(state)\n",
    "\n",
    "    # select a random action wih probability eps\n",
    "    if random.random() <= eps:\n",
    "        action = np.random.randint(0, env.action_space.n)\n",
    "    else:\n",
    "        action = np.argmax(values.cpu().numpy())\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def train(batch_size, current, target, optim, memory, gamma):\n",
    "\n",
    "    states, actions, next_states, rewards, is_done = memory.sample(batch_size)\n",
    "\n",
    "    q_values = current(states)\n",
    "\n",
    "    next_q_values = current(next_states)\n",
    "    next_q_state_values = target(next_states)\n",
    "\n",
    "    q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value = next_q_state_values.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n",
    "    expected_q_value = rewards + gamma * next_q_value * (1 - is_done)\n",
    "\n",
    "    loss = (q_value - expected_q_value.detach()).pow(2).mean()\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "\n",
    "def evaluate(Qmodel, env, repeats):\n",
    "    \"\"\"\n",
    "    Runs a greedy policy with respect to the current Q-Network for \"repeats\" many episodes. Returns the average\n",
    "    episode reward.\n",
    "    \"\"\"\n",
    "    Qmodel.eval()\n",
    "    perform = 0\n",
    "    for _ in range(repeats):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state = torch.Tensor(state).to(device)\n",
    "            with torch.no_grad():\n",
    "                values = Qmodel(state)\n",
    "            action = np.argmax(values.cpu().numpy())\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            perform += reward\n",
    "    Qmodel.train()\n",
    "    return perform/repeats\n",
    "\n",
    "\n",
    "def update_parameters(current_model, target_model):\n",
    "    target_model.load_state_dict(current_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      "rewards:  9.43\n",
      "lr:  0.001\n",
      "eps:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MIBOUN\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  100\n",
      "rewards:  78.16\n",
      "lr:  0.0009000000000000001\n",
      "eps:  0.6057704364907278\n",
      "Episode:  200\n",
      "rewards:  200.0\n",
      "lr:  0.0008100000000000001\n",
      "eps:  0.3669578217261671\n",
      "Episode:  300\n",
      "rewards:  200.0\n",
      "lr:  0.000729\n",
      "eps:  0.22229219984074702\n",
      "Episode:  400\n",
      "rewards:  197.88\n",
      "lr:  0.0006561000000000001\n",
      "eps:  0.1346580429260134\n",
      "Episode:  500\n",
      "rewards:  199.24\n",
      "lr:  0.00059049\n",
      "eps:  0.08157186144027828\n",
      "Episode:  600\n",
      "rewards:  199.54\n",
      "lr:  0.000531441\n",
      "eps:  0.0494138221100385\n",
      "Episode:  700\n",
      "rewards:  199.42\n",
      "lr:  0.0004782969\n",
      "eps:  0.029933432588273214\n",
      "Episode:  800\n",
      "rewards:  195.81\n",
      "lr:  0.00043046721\n",
      "eps:  0.018132788524664028\n",
      "Episode:  900\n",
      "rewards:  197.56\n",
      "lr:  0.000387420489\n",
      "eps:  0.01098430721937979\n",
      "Episode:  1000\n",
      "rewards:  196.65\n",
      "lr:  0.0003486784401\n",
      "eps:  0.01\n",
      "Episode:  1100\n",
      "rewards:  199.56\n",
      "lr:  0.00031381059609000004\n",
      "eps:  0.01\n",
      "Episode:  1200\n",
      "rewards:  179.52\n",
      "lr:  0.00028242953648100003\n",
      "eps:  0.01\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Q_1 = QNetwork(action_dim=env.action_space.n, state_dim=env.observation_space.shape[0],\n",
    "                                    hidden_dim=hidden_dim).to(device)\n",
    "Q_2 = QNetwork(action_dim=env.action_space.n, state_dim=env.observation_space.shape[0],\n",
    "                                    hidden_dim=hidden_dim).to(device)\n",
    "# transfer parameters from Q_1 to Q_2\n",
    "update_parameters(Q_1, Q_2)\n",
    "\n",
    "# we only train Q_1\n",
    "for param in Q_2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam(Q_1.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=lr_step, gamma=lr_gamma)\n",
    "\n",
    "memory = Memory(max_memory_size)\n",
    "performance = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # display the performance\n",
    "    if episode % measure_step == 0:\n",
    "        performance.append([episode, evaluate(Q_1, env, measure_repeats)])\n",
    "        print(\"Episode: \", episode)\n",
    "        print(\"rewards: \", performance[-1][1])\n",
    "        print(\"lr: \", scheduler.get_last_lr()[0])\n",
    "        print(\"eps: \", eps)\n",
    "\n",
    "    state = env.reset()\n",
    "    memory.state.append(state)\n",
    "\n",
    "    done = False\n",
    "    i = 0\n",
    "    while not done:\n",
    "        i += 1\n",
    "        action = select_action(Q_2, env, state, eps)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if i > horizon:\n",
    "            done = True\n",
    "\n",
    "        # render the environment if render == True\n",
    "        if render and episode % render_step == 0:\n",
    "            env.render()\n",
    "\n",
    "        # save state, action, reward sequence\n",
    "        memory.update(state, action, reward, done)\n",
    "\n",
    "    if episode >= min_episodes and episode % update_step == 0:\n",
    "        for _ in range(update_repeats):\n",
    "            train(batch_size, Q_1, Q_2, optimizer, memory, gamma)\n",
    "\n",
    "        # transfer new parameter from Q_1 to Q_2\n",
    "        update_parameters(Q_1, Q_2)\n",
    "\n",
    "    # update learning rate and eps\n",
    "    scheduler.step()\n",
    "    eps = max(eps*eps_decay, eps_min)\n",
    "\n",
    "return Q_1, performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
